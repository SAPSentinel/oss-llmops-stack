# litellm config yml

model_list:
  - model_name: "*"  # Default fallback model
    litellm_params:
      model: "ollama/nomic-embed-text"
      api_base: "http://192.168.1.13:11434"

litellm_settings: #
  drop_params: True
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  langfuse_default_tags: ["cache_hit", "cache_key", "proxy_base_url", "user_api_key_alias", "user_api_key_user_id", "user_api_key_user_email", "user_api_key_team_alias", "semantic-similarity", "proxy_base_url"]
  set_verbose: true
  # CRITICAL: These tell LiteLLM to pass metadata to Langfuse
  langfuse_default_tags: [
    "cache_hit", 
    "cache_key", 
    "proxy_base_url", 
    "user_api_key_alias", 
    "user_api_key_user_id", 
    "user_api_key_user_email", 
    "user_api_key_team_alias", 
    "semantic-similarity"
  ]
  # Caching settings - FIXED Redis host
  cache: true
  cache_params:
    type: redis
    host: "192.168.1.13"  # ✅ REMOVED http://
    port: 6379
    password: "myredissecret"
    namespace: "litellm.caching.caching"
    max_connections: 100


general_settings:
  master_key: "sk-1234"
  alerting: ["slack"]
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: internal_user #changed from customer
# Data persistence
  store_model_in_db: true
  store_prompts_in_spend_logs: true

# CRITICAL: Enable message logging for callbacks
callback_settings:
  langfuse:
    pass_metadata: true  # ✅ THIS IS KEY - passes session_id to Langfuse
  otel:
    message_logging: true
