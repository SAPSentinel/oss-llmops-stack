# litellm config yml

model_list:
  - model_name: "*"  # Default fallback model
    litellm_params:
      model: "ollama/nomic-embed-text"
      api_base: "http://192.168.1.13:11434"

litellm_settings: #
  drop_params: True
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  set_verbose: true

  # Caching settings - FIXED Redis host
  cache: true
  cache_params:
    type: redis
    host: "192.168.1.13"  # ✅ REMOVED http://
    port: 6379
    password: "myredissecret"
    namespace: "litellm.caching.caching"
    max_connections: 100


general_settings:
  master_key: "sk-1234"
  
  # CRITICAL: Add Langfuse credentials here
  langfuse_public_key: "pk-lf-ossllmopsstack"
  langfuse_secret_key: "sk-lf-ossllmopsstack"
  langfuse_host: "http://192.168.1.13:3002"
  
  
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: internal_user #changed from customer
# Data persistence
  store_model_in_db: true
  store_prompts_in_spend_logs: true

# CRITICAL: Enable message logging for callbacks
callback_settings:
  langfuse:
    pass_metadata: true  # ✅ THIS IS KEY - passes session_id to Langfuse
  otel:
    message_logging: true
