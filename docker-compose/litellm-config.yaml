# litellm config yml

model_list:
  - model_name: "*" # all requests where model not in your config go to this deployment
    litellm_params:
      model: "ollama/nomic-embed-text" # set `openai/` to use the openai route
      api_base: "http://192.168.1.13:11434"

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env
  langfuse_default_tags: ["cache_hit", "cache_key", "proxy_base_url", "user_api_key_alias", "user_api_key_user_id", "user_api_key_user_email", "user_api_key_team_alias", "semantic-similarity", "proxy_base_url"]# Adding default tags to Langfuse logs ["proxy_base_url", "user_api_key_alias", "cache_hit"]

general_settings:
  master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: customer

  store_model_in_db: true
  store_prompts_in_spend_logs: true

guardrails:
  - guardrail_name: "guardrails_ai-guard"
    litellm_params:
      guardrail: guardrails_ai
      guard_name: "detect-secrets-guard"            # ðŸ‘ˆ Guardrail AI guard name
      mode: "pre_call"
      guardrails_ai_api_input_format: "llmOutput"   # ðŸ‘ˆ This is the only option that currently works (and it is a default), use it for both pre_call and post_call hooks
      api_base: http://192.168.1.13:8780   # ðŸ‘ˆ Guardrails AI API Base. Defaults to "http://0.0.0.0:8000" os.environ/GUARDRAILS_AI_API_BASE
