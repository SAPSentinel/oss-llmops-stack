# litellm config yml

model_list:
  - model_name: "*"  # Default fallback model
    litellm_params:
      model: "ollama/nomic-embed-text"
      api_base: "http://192.168.1.13:11434"

litellm_settings: #
  drop_params: True
  success_callback: ["langfuse"]
  langfuse_default_tags: ["cache_hit", "cache_key", "proxy_base_url", "user_api_key_alias", "user_api_key_user_id", "user_api_key_user_email", "user_api_key_team_alias", "semantic-similarity", "proxy_base_url"]

# Caching settings
  cache: true
  cache_params:
    type: redis
    host: "http://192.168.1.13"
    port: 6379
    password: "myredissecret"
    namespace: "litellm.caching.caching"
    max_connections: 100

general_settings:
  master_key: "sk-1234"
  alerting: ["slack"]
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: internal_user #changed from customer
# Data persistence
  store_model_in_db: true
  store_prompts_in_spend_logs: true

callback_settings:
  otel:
    message_logging: true

guardrails:
  - guardrail_name: "guardrails_ai-guard"
    litellm_params:
      guardrail: guardrails_ai
      guard_name: "detect-secrets-guard"            # ðŸ‘ˆ Guardrail AI guard name
      mode: "pre_call"
      guardrails_ai_api_input_format: "llmOutput"   # ðŸ‘ˆ This is the only option that currently works (and it is a default), use it for both pre_call and post_call hooks
      api_base: http://192.168.1.13:8780   # ðŸ‘ˆ Guardrails AI API Base. Defaults to "http://0.0.0.0:8000" os.environ/GUARDRAILS_AI_API_BASE
