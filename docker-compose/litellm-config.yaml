# litellm config yml

model_list:
  - model_name: "*" # all requests where model not in your config go to this deployment
    litellm_params:
      model: "ollama/nomic-embed-text" # set `openai/` to use the openai route
      api_base: "http://192.168.1.7:11434"

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env
  langfuse_default_tags: ["proxy_base_url", "user_api_key_alias", "cache_hit"] # Adding default tags to Langfuse logs

general_settings:
  master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: customer

  store_model_in_db: true
  store_prompts_in_spend_logs: true
